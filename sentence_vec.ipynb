{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building Dense Vectors Using Transformers\n",
    "We will be using the sentence-transformers/stsb-distilbert-base model to build our dense vectors.\n",
    "\n",
    "https://github.com/jamescalam/transformers/blob/main/course/similarity/02_similarity_metrics.ipynb => metrics similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/stsb-distilbert-base'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/489 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5193acb4831436ab3d473ae09a3307d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe388edd80394a9e85289c0288e5eafb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05b8498db621405da3a6ce81052f957e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8888abb407524d9cafec2d0039a139be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "627885b50cd543df9aa08fc947c34e99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8e9178acdf94259b38615081e516518"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perbedaan antara kedua metode ini adalah bahwa encode_plus memungkinkan pengguna untuk memasukkan lebih banyak parameter seperti max_length, truncation, dan padding. sedangkan Encode adalah proses mengubah data atau informasi menjadi bentuk yang dapat diproses oleh komputer atau sistem lainnya."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "text = \"hello world what a time to be alive!\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(text, max_length=128,\n",
    "                               truncation=True, padding='max_length',\n",
    "                               return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 7592, 2088, 2054, 1037, 2051, 2000, 2022, 4142,  999,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseModelOutput(last_hidden_state=tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n         ...,\n         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens) # ** asteris 2 merepresentasikan tipe data dict yang memiliki key variabel\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "outputs.last_hidden_state adalah tensor yang berisi representasi token terakhir dari setiap sequence dalam batch. Representasi ini dapat digunakan sebagai input untuk tugas downstream seperti klasifikasi teks atau NER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n         ...,\n         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 128, 768])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128]),\n tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape, attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128, 1]),\n tensor([[[1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]]]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).shape, attention_mask.unsqueeze(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128, 768]),\n tensor([[[1, 1, 1,  ..., 1, 1, 1],\n          [1, 1, 1,  ..., 1, 1, 1],\n          [1, 1, 1,  ..., 1, 1, 1],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).expand(embeddings.shape).shape, attention_mask.unsqueeze(-1).expand(embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.shape).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Masked Embedding pada NLP adalah representasi vektor dari kata-kata dalam dokumen teks yang digunakan untuk memproses teks dalam bentuk numerik dengan memasukkan token khusus yang menunjukkan posisi di mana token tersebut harus disembunyikan1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128, 768]),\n tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n          [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n          [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n          ...,\n          [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n        grad_fn=<MulBackward0>))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape, masked_embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fungsi dari program torch.sum adalah untuk menjumlahkan nilai-nilai pada tensor masked_embeddings pada dimensi ke-1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 768]),\n tensor([[-4.2333e+00,  8.5918e+00, -1.9492e+00, -1.5538e+01, -2.5694e+00,\n           9.9980e+00, -8.6229e-01,  6.6381e+00,  7.4674e-01, -4.3826e+00,\n           4.3145e-01, -5.1452e+00, -7.9033e+00, -1.3049e+00, -7.8313e-01,\n           7.2620e-02, -1.5502e+00,  3.4398e+00, -7.2131e+00, -1.8117e+00,\n          -1.1028e+00, -4.2192e+00,  6.7406e-01, -8.1200e+00, -6.5910e+00,\n           3.1741e+00,  9.5010e+00,  6.4226e+00, -3.8564e-01,  4.7517e+00,\n          -5.5122e+00, -4.7848e+00,  2.5848e+00, -4.0840e+00, -1.1049e+01,\n           1.1000e+01, -2.3100e+01, -3.5476e+00, -1.7694e+00, -8.1072e+00,\n           6.0421e+00, -1.3273e+00,  7.9968e+00, -5.5361e-01, -1.9299e+01,\n           8.8631e+00, -5.5608e+00, -5.1889e+00, -1.8400e+00,  6.5700e+00,\n           6.1823e+00, -3.9714e+00, -1.8072e+00, -6.0736e+00, -5.5459e+00,\n           6.1806e+00, -1.2557e+01,  1.1848e+01,  6.1258e+00, -7.7695e-01,\n          -2.9625e+00, -7.5796e+00,  1.9902e+00,  3.4150e+00,  4.2940e-01,\n           3.4171e+00, -4.8944e+00, -5.2099e-01,  1.8711e+01,  1.3580e+00,\n           7.4183e+00, -7.0588e+00,  5.1005e+00, -1.2008e+01, -8.3979e+00,\n          -3.4005e+00, -3.3634e+00,  8.4671e+00, -1.4859e+01, -2.5647e+00,\n          -3.7282e+00, -7.9088e+00, -1.5737e+00,  6.1633e+00, -7.5532e+00,\n          -6.8546e+00, -1.8338e+00,  5.3404e+00, -4.5397e+00,  1.1818e+01,\n           8.4247e+00,  3.6191e+00,  4.0853e+00,  4.9370e+00,  1.0839e+01,\n           3.2002e+00, -9.4209e+00, -8.4246e+00,  3.8081e+00,  1.9417e+00,\n           8.5963e-01, -2.6237e+00,  8.1371e+00, -3.6967e+00, -1.4599e+00,\n          -3.3402e-01,  1.0361e+01, -1.0608e+00, -4.6787e+00, -3.9601e+00,\n           4.5047e+00, -3.9128e+00, -6.7495e+00,  1.1269e+01, -1.8640e+00,\n           6.3818e+00, -3.9189e+00, -2.0845e+00, -1.5038e+01, -4.6650e-01,\n           8.3894e+00, -4.2362e+00, -4.8983e+00,  6.6915e+00, -3.8679e+00,\n          -3.4821e+00,  5.9643e+00, -4.7056e-01,  2.0128e+01, -5.5062e-01,\n           4.4362e+00,  4.2783e+00, -1.5927e+00, -5.5561e+00,  5.4618e+00,\n           2.4447e+00, -1.8707e+00,  1.7616e+00, -6.2107e+00, -6.7396e+00,\n           7.1870e+00,  8.7307e+00, -1.4695e+00, -3.2607e+00,  7.2050e+00,\n           5.0132e+00,  1.4324e+00,  9.4581e+00,  1.0504e+00, -4.5254e+00,\n          -6.6871e+00, -7.5895e+00, -8.8719e+00, -3.6668e+00, -6.7820e+00,\n          -7.3881e+00, -8.7085e+00, -6.6976e+00, -7.4626e+00,  8.4997e+00,\n          -2.0218e+00,  4.7239e+00,  9.0736e+00, -3.4757e+00, -1.8043e+01,\n           9.2437e+00,  6.8977e+00,  5.0192e+00,  3.9855e+00, -4.9721e+00,\n          -2.0037e-01,  6.6961e-01, -8.2838e+00,  1.1490e+01, -1.3290e+01,\n          -3.7307e+00,  6.2184e+00, -9.3610e+00, -3.8265e+00,  1.4141e+00,\n          -6.3897e+00, -5.8092e+00, -1.3662e-01,  7.0269e-01, -7.4225e-01,\n          -1.9097e+00,  7.8034e+00, -1.5506e+01, -1.5348e+00, -2.1904e+00,\n           5.2483e+00,  8.8399e+00,  4.1078e+00, -5.8150e+00,  4.3601e+00,\n           1.0437e+01,  1.3714e+00, -1.9675e+00,  8.7954e+00,  4.0343e-01,\n           3.5262e+00,  1.1163e+01,  7.1325e+00, -1.7896e+00,  3.6917e+00,\n          -5.5164e+00,  2.6524e+00, -9.8904e+00, -1.3762e+00, -3.7066e+00,\n           3.3768e+00, -2.4533e+00, -4.2895e+00, -3.1791e+00, -4.2306e+00,\n           9.7396e+00, -1.8661e+00,  2.7080e+00, -9.2527e+00, -3.6092e+00,\n          -3.6000e+00, -2.4452e-01,  4.9644e+00, -4.5393e+00,  1.0114e+01,\n          -6.9729e+00,  1.0365e+01,  1.5264e+00, -2.8491e-01, -4.9710e+00,\n           6.2756e-01,  2.7214e+00, -6.3785e+00,  8.5464e-01,  8.4052e-01,\n          -3.7407e+00,  4.7660e+00, -4.3160e+00,  4.9649e+00, -5.9317e+00,\n          -6.3402e+00, -5.6309e+00, -2.7321e-01, -3.2935e+00, -2.8730e+00,\n          -7.5061e+00, -1.1908e+00,  3.0657e+00,  5.7582e+00, -1.4069e+01,\n          -8.3493e+00, -4.5694e+00,  3.0605e+00,  1.4137e+00,  8.5841e+00,\n          -8.9095e+00, -7.9655e+00, -9.5471e+00, -9.4333e+00,  1.9353e+00,\n           4.4091e+00, -8.0736e+00,  4.2902e+00,  1.2367e+00,  1.3298e+00,\n          -1.1872e+00,  1.2455e+00, -3.0568e+00, -8.7818e-02,  2.4180e-01,\n          -9.2073e+00,  3.7763e+00,  1.4897e+00,  6.3717e-01,  2.1579e+01,\n           9.1753e+00, -8.0214e+00,  5.0776e+00, -3.3392e+00, -3.7582e+00,\n          -7.6841e+00,  1.0440e+00, -1.2580e+01,  4.7106e-01, -3.7408e+00,\n           1.9794e+00, -6.6529e+00, -1.0608e+01, -5.4200e+00, -1.5252e+00,\n          -1.6512e+01,  4.1977e+00, -4.1627e+00, -3.0291e+00,  8.8935e-01,\n           3.3436e+00,  7.0303e-02, -1.1331e+01, -8.1481e+00,  6.8713e-01,\n           2.5617e+00, -1.0289e+01,  1.1086e+00, -1.3331e+01, -7.3512e+00,\n          -6.1419e-01, -1.0175e+00, -5.4001e+00,  1.2710e+00,  4.9979e+00,\n           1.3458e+01,  6.3269e+00, -1.1008e+01,  6.8390e+00, -7.3562e-01,\n           1.3926e+01,  5.4262e+00, -1.7005e+00,  1.0229e+00, -6.2699e-01,\n          -3.1414e-01,  5.8007e+00, -3.3730e+00, -8.8422e+00, -1.8321e+00,\n          -6.1445e+00, -5.6884e+00,  5.1601e-01, -1.2572e+01, -1.7872e+00,\n          -1.3572e+00, -3.8158e+00, -3.8749e+00, -4.2609e+00,  1.1433e+01,\n          -1.8118e+00, -1.5271e+00,  1.0096e+01, -8.0033e+00, -1.6703e+00,\n           3.1572e+00, -2.3064e+00,  6.1099e+00,  2.0838e+00, -5.5374e+00,\n          -1.1986e+01, -8.2976e+00, -1.4988e+01, -4.2650e-01, -8.5472e+00,\n           2.7961e+00,  3.4826e+00, -9.4832e+00, -2.5976e+00,  5.1130e-02,\n           5.6953e+00,  5.2003e+00, -8.1269e-01, -6.1367e+00,  2.6447e+00,\n           1.0456e+01,  2.9287e-01, -5.1264e+00, -3.6724e+00,  4.9390e+00,\n           1.2115e+01,  6.5286e+00, -1.3268e+00,  2.3158e+00,  8.0408e+00,\n          -2.6106e-01,  2.8984e+00,  3.7249e+00,  6.5508e+00, -3.6792e+00,\n           1.3798e+00,  3.6329e+00, -1.7267e+00, -4.1725e+00, -2.7586e+00,\n          -3.2444e+00,  9.0851e-01,  2.0214e+00, -7.5054e+00, -9.4961e-01,\n          -6.3581e+00, -9.3175e-01, -1.5565e+00,  1.0077e+01, -6.2435e+00,\n          -1.2092e+00, -1.7486e-01,  6.9226e+00,  2.3791e+00,  1.2387e+00,\n           7.2411e+00,  5.0199e+00,  1.2030e+01,  8.1703e+00, -4.1047e+00,\n           4.1059e+00,  1.1890e+01, -2.6783e+00, -5.4034e+00,  1.2861e+01,\n           1.1128e+01, -2.4397e+00, -9.2404e-01,  4.9292e+00,  9.2075e+00,\n          -1.6414e+00, -8.0828e-01,  3.1206e+00,  5.8567e+00,  3.9054e-01,\n          -8.0242e+00,  2.4513e+00, -8.9864e+00, -1.0814e+00,  1.8466e-01,\n          -1.1066e+01,  6.4731e-01, -1.2906e+00, -2.7529e-02,  1.0763e+01,\n           4.7292e+00,  6.0685e+00,  8.3341e+00,  4.5807e+00, -8.5667e+00,\n           7.2438e+00, -2.9814e+00, -2.3314e+00,  2.9520e+00,  9.8178e-01,\n          -2.4229e+00,  7.7061e+00, -5.5596e+00, -1.0209e+01,  3.1347e+00,\n          -3.6200e+00,  9.9178e+00,  6.1809e+00,  4.2327e-01,  8.3711e+00,\n          -2.6669e-01, -5.7756e+00,  5.4167e+00, -1.2456e+01, -8.7337e-01,\n           9.8223e+00, -4.5143e+00, -4.6846e+00,  6.1917e+00, -3.4233e+00,\n          -5.5414e-01, -8.5751e+00,  7.7341e+00, -1.2367e+00,  3.4801e+00,\n           1.7579e+00, -1.0583e-01, -1.1421e+01, -2.0622e-02, -1.0944e+00,\n          -1.6644e+01,  8.3290e+00, -8.5573e-01,  1.1351e+01,  5.7346e+00,\n          -3.5290e+00, -1.5110e+00,  1.1928e+01, -4.1412e+00, -3.8115e-01,\n           1.4407e+00, -3.8703e+00, -8.9571e+00,  3.7607e+00,  7.4010e-01,\n          -2.1093e+00,  2.4475e+00,  4.4869e+00, -4.4189e-01,  1.0273e+01,\n           1.6333e+00,  1.0356e+00, -6.7673e+00,  1.9881e+00, -1.0326e+01,\n          -3.9385e+00, -1.2580e+00,  1.0825e+01, -1.5132e+01,  1.0720e+00,\n          -2.5574e+00,  1.0452e+01, -2.3533e+00, -2.5733e+00,  5.5260e+00,\n           2.8488e+00, -2.5094e+00, -2.6048e+00, -2.5467e+00, -1.0786e+01,\n           4.3019e+00,  6.4147e+00,  1.8206e+00, -4.0094e+00,  4.6282e+00,\n           1.0294e+01, -5.1454e+00, -2.4932e-01,  4.1721e+00, -6.7238e+00,\n          -4.4804e+00,  9.9831e+00,  1.1575e+01, -2.1640e+00, -6.6471e-01,\n          -6.1229e-01,  4.2504e+00, -1.4034e+00,  6.7774e+00, -4.3150e+00,\n           9.9650e+00, -4.7890e+00, -8.1577e-01,  1.1931e+00, -7.0421e-01,\n           6.8506e+00,  4.1797e+00, -1.2737e+00, -2.1243e+01,  7.8255e+00,\n           1.2926e+00, -4.5712e+00, -8.4971e+00,  7.0061e+00,  5.8407e+00,\n           1.0688e+00, -7.5740e-01, -9.7627e+00,  4.6204e+00,  1.6209e+00,\n           4.9444e+00,  1.1832e+00,  9.4233e+00,  2.4084e+00, -1.6077e+00,\n          -2.3263e+00,  8.0400e-01,  6.2422e+00,  4.3358e+00,  3.1220e-01,\n           1.1462e+01, -1.0917e+00, -6.0637e+00,  8.0973e-01,  1.2948e+01,\n          -6.0898e+00, -1.1639e+00, -4.6455e+00, -1.7441e+01,  8.1157e+00,\n          -1.5641e+00, -1.1681e+01, -7.5139e+00,  1.4649e-01,  4.5903e+00,\n          -1.2485e+01,  2.7621e+00,  5.4495e+00,  1.1262e+00, -7.9078e+00,\n           1.1676e+00,  8.4519e+00, -6.7009e-01, -4.0530e+00,  5.6214e-01,\n          -7.6305e+00, -1.8014e+00,  8.0292e+00, -2.9900e+00, -1.9221e+00,\n           7.3342e+00, -2.7145e+00, -3.1410e+00, -8.5615e-01,  9.6245e-01,\n           2.3506e+00,  9.6007e+00, -1.0869e+00, -5.5703e+00, -4.7153e+00,\n          -5.7054e+00,  4.6992e+00,  3.4866e+00, -3.2786e+00, -9.1768e+00,\n          -1.1863e+01, -8.5004e+00,  5.4055e+00,  1.2399e+01,  1.5067e-01,\n          -7.5384e-01, -1.2660e+00, -7.2202e+00,  8.7668e+00, -2.8674e+00,\n           1.1435e+01, -5.2770e+00, -1.5883e+00,  8.5895e+00, -6.4959e+00,\n           2.3762e+00, -8.1894e+00, -1.4661e+00, -1.6075e+00,  9.8797e+00,\n          -1.1138e+01, -7.2118e+00,  8.4338e+00, -3.1261e+00, -1.5268e+00,\n          -7.9140e+00, -5.6957e+00,  1.5745e+00, -2.5888e+00, -6.5830e+00,\n           6.6478e-01, -6.9504e-01, -1.7230e+01, -2.1498e+00,  2.2450e+00,\n          -1.1371e+01,  1.0034e+01,  2.6347e+00,  1.1969e+00, -2.2050e+00,\n           9.3077e+00, -8.2522e-01,  3.8266e+00, -1.6603e+01, -2.7543e+00,\n          -7.1539e-01,  7.6598e+00, -2.9447e+00,  9.8681e-01, -5.3738e+00,\n           7.7962e+00, -8.4476e+00,  9.3486e+00,  4.5520e+00, -4.4506e+00,\n           3.1549e+00,  1.1531e+01,  1.7976e+00,  9.8394e-01, -3.2053e+00,\n          -6.6656e+00, -1.1168e+00, -3.6338e+00, -4.8277e+00, -7.5961e-01,\n           5.6037e-01,  4.1474e+00,  7.3579e-01,  6.4209e+00,  3.5635e+00,\n          -2.7481e+00, -3.2495e+00,  4.1721e+00, -3.4309e+00, -1.4586e+00,\n           5.5000e+00,  7.0697e+00, -3.4015e+00, -3.2605e+00, -9.7275e+00,\n           4.4671e+00,  7.1568e+00, -1.3396e-01,  1.2808e+01, -6.5392e+00,\n          -1.9870e+00,  6.7853e+00, -1.2399e+00, -6.6896e+00, -7.9313e+00,\n           8.4290e+00, -3.6291e+00, -5.5314e+00,  5.6719e+00, -4.5728e+00,\n          -5.2474e+00, -3.1593e+00,  9.2926e+00, -4.3079e+00,  1.0154e+00,\n           8.6027e+00,  2.3390e+00, -7.1067e+00,  1.0583e+01,  1.3476e+00,\n          -1.0989e+01,  1.6441e+00, -2.7727e+00, -4.3540e+00,  9.4260e+00,\n           3.8937e+00, -9.6490e+00,  2.5967e+00, -2.3664e+00, -6.7569e+00,\n          -1.6380e+00,  4.6384e+00, -1.0311e+01,  7.8164e+00, -1.6748e+00,\n           1.4653e+01,  1.4172e+00,  1.5498e+00,  7.4662e+00, -6.3804e+00,\n           1.6784e+00,  6.1519e+00, -1.5836e+00, -3.7473e+00, -6.8902e+00,\n           1.6730e+00,  1.2198e+00,  7.0708e-01,  6.0613e+00, -1.2994e+00,\n          -5.1265e+00,  3.4018e+00, -3.8754e+00,  4.5725e+00,  1.5944e+01,\n          -3.8798e+00, -1.0189e+01, -3.3502e+00, -3.1457e+00, -5.3708e+00,\n          -2.2090e+00,  6.8482e-01, -6.9674e+00,  4.8127e-01,  3.8631e+00,\n           8.3255e+00, -3.4676e+00,  3.2939e+00, -8.2756e+00,  1.3999e+00,\n          -5.6819e+00, -1.9684e-01,  4.4755e+00, -2.1345e+00,  4.0882e+00,\n           6.6228e+00,  1.0776e+01,  1.5279e+00,  6.8577e+00,  4.5445e+00,\n           4.5509e+00,  6.2232e+00, -5.1490e+00]], grad_fn=<SumBackward1>))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape, summed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fungsi dari program tersebut adalah untuk membatasi nilai tensor mask.sum(1) pada rentang nilai minimum 1e-9 dan nilai maksimum yang tidak terbatas.\n",
    "\n",
    "1e-9 adalah notasi ilmiah yang merepresentasikan bilangan pecahan 0.000000001 atau 1 dibagi dengan 1 milyar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that is how we calculate our dense similarity vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-3.8485e-01,  7.8107e-01, -1.7720e-01, -1.4125e+00, -2.3358e-01,\n          9.0891e-01, -7.8390e-02,  6.0347e-01,  6.7885e-02, -3.9841e-01,\n          3.9223e-02, -4.6774e-01, -7.1848e-01, -1.1863e-01, -7.1193e-02,\n          6.6018e-03, -1.4093e-01,  3.1271e-01, -6.5574e-01, -1.6470e-01,\n         -1.0026e-01, -3.8357e-01,  6.1278e-02, -7.3818e-01, -5.9918e-01,\n          2.8855e-01,  8.6372e-01,  5.8388e-01, -3.5058e-02,  4.3197e-01,\n         -5.0111e-01, -4.3498e-01,  2.3498e-01, -3.7127e-01, -1.0044e+00,\n          1.0000e+00, -2.1000e+00, -3.2251e-01, -1.6085e-01, -7.3701e-01,\n          5.4928e-01, -1.2066e-01,  7.2698e-01, -5.0328e-02, -1.7545e+00,\n          8.0573e-01, -5.0553e-01, -4.7172e-01, -1.6727e-01,  5.9727e-01,\n          5.6203e-01, -3.6104e-01, -1.6429e-01, -5.5215e-01, -5.0418e-01,\n          5.6187e-01, -1.1415e+00,  1.0771e+00,  5.5689e-01, -7.0632e-02,\n         -2.6932e-01, -6.8905e-01,  1.8093e-01,  3.1045e-01,  3.9036e-02,\n          3.1064e-01, -4.4495e-01, -4.7363e-02,  1.7010e+00,  1.2346e-01,\n          6.7439e-01, -6.4171e-01,  4.6369e-01, -1.0917e+00, -7.6344e-01,\n         -3.0914e-01, -3.0577e-01,  7.6974e-01, -1.3508e+00, -2.3315e-01,\n         -3.3893e-01, -7.1898e-01, -1.4306e-01,  5.6030e-01, -6.8665e-01,\n         -6.2314e-01, -1.6671e-01,  4.8549e-01, -4.1270e-01,  1.0744e+00,\n          7.6589e-01,  3.2901e-01,  3.7139e-01,  4.4882e-01,  9.8538e-01,\n          2.9093e-01, -8.5644e-01, -7.6587e-01,  3.4619e-01,  1.7652e-01,\n          7.8148e-02, -2.3852e-01,  7.3973e-01, -3.3606e-01, -1.3271e-01,\n         -3.0366e-02,  9.4190e-01, -9.6432e-02, -4.2533e-01, -3.6001e-01,\n          4.0952e-01, -3.5571e-01, -6.1359e-01,  1.0244e+00, -1.6946e-01,\n          5.8016e-01, -3.5626e-01, -1.8950e-01, -1.3671e+00, -4.2409e-02,\n          7.6267e-01, -3.8510e-01, -4.4530e-01,  6.0831e-01, -3.5163e-01,\n         -3.1655e-01,  5.4221e-01, -4.2779e-02,  1.8298e+00, -5.0056e-02,\n          4.0329e-01,  3.8894e-01, -1.4479e-01, -5.0510e-01,  4.9653e-01,\n          2.2225e-01, -1.7006e-01,  1.6015e-01, -5.6461e-01, -6.1269e-01,\n          6.5336e-01,  7.9370e-01, -1.3359e-01, -2.9642e-01,  6.5500e-01,\n          4.5574e-01,  1.3021e-01,  8.5983e-01,  9.5493e-02, -4.1140e-01,\n         -6.0792e-01, -6.8995e-01, -8.0654e-01, -3.3334e-01, -6.1655e-01,\n         -6.7164e-01, -7.9168e-01, -6.0887e-01, -6.7842e-01,  7.7270e-01,\n         -1.8380e-01,  4.2944e-01,  8.2487e-01, -3.1597e-01, -1.6402e+00,\n          8.4033e-01,  6.2706e-01,  4.5629e-01,  3.6232e-01, -4.5201e-01,\n         -1.8216e-02,  6.0874e-02, -7.5308e-01,  1.0445e+00, -1.2082e+00,\n         -3.3916e-01,  5.6531e-01, -8.5100e-01, -3.4787e-01,  1.2856e-01,\n         -5.8088e-01, -5.2811e-01, -1.2420e-02,  6.3881e-02, -6.7477e-02,\n         -1.7361e-01,  7.0940e-01, -1.4097e+00, -1.3953e-01, -1.9912e-01,\n          4.7711e-01,  8.0363e-01,  3.7343e-01, -5.2863e-01,  3.9637e-01,\n          9.4878e-01,  1.2467e-01, -1.7886e-01,  7.9958e-01,  3.6676e-02,\n          3.2056e-01,  1.0148e+00,  6.4841e-01, -1.6269e-01,  3.3561e-01,\n         -5.0149e-01,  2.4113e-01, -8.9913e-01, -1.2511e-01, -3.3696e-01,\n          3.0698e-01, -2.2303e-01, -3.8996e-01, -2.8901e-01, -3.8460e-01,\n          8.8542e-01, -1.6964e-01,  2.4618e-01, -8.4116e-01, -3.2811e-01,\n         -3.2727e-01, -2.2229e-02,  4.5131e-01, -4.1267e-01,  9.1945e-01,\n         -6.3390e-01,  9.4229e-01,  1.3876e-01, -2.5901e-02, -4.5191e-01,\n          5.7051e-02,  2.4740e-01, -5.7986e-01,  7.7694e-02,  7.6410e-02,\n         -3.4006e-01,  4.3327e-01, -3.9236e-01,  4.5135e-01, -5.3925e-01,\n         -5.7638e-01, -5.1190e-01, -2.4838e-02, -2.9940e-01, -2.6119e-01,\n         -6.8237e-01, -1.0826e-01,  2.7870e-01,  5.2347e-01, -1.2790e+00,\n         -7.5903e-01, -4.1540e-01,  2.7823e-01,  1.2852e-01,  7.8037e-01,\n         -8.0996e-01, -7.2413e-01, -8.6791e-01, -8.5757e-01,  1.7594e-01,\n          4.0083e-01, -7.3397e-01,  3.9002e-01,  1.1243e-01,  1.2089e-01,\n         -1.0793e-01,  1.1323e-01, -2.7789e-01, -7.9834e-03,  2.1982e-02,\n         -8.3703e-01,  3.4330e-01,  1.3543e-01,  5.7925e-02,  1.9617e+00,\n          8.3412e-01, -7.2922e-01,  4.6160e-01, -3.0357e-01, -3.4166e-01,\n         -6.9856e-01,  9.4905e-02, -1.1436e+00,  4.2824e-02, -3.4008e-01,\n          1.7994e-01, -6.0481e-01, -9.6435e-01, -4.9273e-01, -1.3865e-01,\n         -1.5011e+00,  3.8161e-01, -3.7843e-01, -2.7537e-01,  8.0850e-02,\n          3.0396e-01,  6.3912e-03, -1.0301e+00, -7.4074e-01,  6.2466e-02,\n          2.3288e-01, -9.3533e-01,  1.0078e-01, -1.2119e+00, -6.6829e-01,\n         -5.5835e-02, -9.2504e-02, -4.9092e-01,  1.1554e-01,  4.5436e-01,\n          1.2235e+00,  5.7517e-01, -1.0007e+00,  6.2173e-01, -6.6875e-02,\n          1.2660e+00,  4.9329e-01, -1.5459e-01,  9.2994e-02, -5.6999e-02,\n         -2.8558e-02,  5.2734e-01, -3.0664e-01, -8.0383e-01, -1.6655e-01,\n         -5.5859e-01, -5.1713e-01,  4.6910e-02, -1.1429e+00, -1.6247e-01,\n         -1.2338e-01, -3.4689e-01, -3.5227e-01, -3.8736e-01,  1.0393e+00,\n         -1.6471e-01, -1.3882e-01,  9.1784e-01, -7.2758e-01, -1.5185e-01,\n          2.8702e-01, -2.0967e-01,  5.5545e-01,  1.8944e-01, -5.0340e-01,\n         -1.0897e+00, -7.5433e-01, -1.3625e+00, -3.8772e-02, -7.7702e-01,\n          2.5419e-01,  3.1660e-01, -8.6211e-01, -2.3615e-01,  4.6482e-03,\n          5.1776e-01,  4.7276e-01, -7.3881e-02, -5.5788e-01,  2.4043e-01,\n          9.5054e-01,  2.6625e-02, -4.6603e-01, -3.3385e-01,  4.4900e-01,\n          1.1014e+00,  5.9350e-01, -1.2061e-01,  2.1053e-01,  7.3098e-01,\n         -2.3733e-02,  2.6349e-01,  3.3863e-01,  5.9553e-01, -3.3448e-01,\n          1.2544e-01,  3.3026e-01, -1.5698e-01, -3.7931e-01, -2.5078e-01,\n         -2.9495e-01,  8.2592e-02,  1.8376e-01, -6.8231e-01, -8.6328e-02,\n         -5.7801e-01, -8.4704e-02, -1.4150e-01,  9.1605e-01, -5.6759e-01,\n         -1.0993e-01, -1.5896e-02,  6.2933e-01,  2.1628e-01,  1.1261e-01,\n          6.5828e-01,  4.5636e-01,  1.0936e+00,  7.4275e-01, -3.7315e-01,\n          3.7326e-01,  1.0809e+00, -2.4348e-01, -4.9122e-01,  1.1691e+00,\n          1.0116e+00, -2.2179e-01, -8.4003e-02,  4.4811e-01,  8.3704e-01,\n         -1.4922e-01, -7.3480e-02,  2.8369e-01,  5.3243e-01,  3.5503e-02,\n         -7.2948e-01,  2.2285e-01, -8.1695e-01, -9.8309e-02,  1.6787e-02,\n         -1.0060e+00,  5.8847e-02, -1.1733e-01, -2.5027e-03,  9.7850e-01,\n          4.2993e-01,  5.5168e-01,  7.5765e-01,  4.1643e-01, -7.7879e-01,\n          6.5853e-01, -2.7104e-01, -2.1195e-01,  2.6836e-01,  8.9253e-02,\n         -2.2026e-01,  7.0055e-01, -5.0542e-01, -9.2811e-01,  2.8497e-01,\n         -3.2909e-01,  9.0162e-01,  5.6190e-01,  3.8479e-02,  7.6101e-01,\n         -2.4245e-02, -5.2505e-01,  4.9243e-01, -1.1323e+00, -7.9398e-02,\n          8.9293e-01, -4.1039e-01, -4.2587e-01,  5.6288e-01, -3.1121e-01,\n         -5.0377e-02, -7.7956e-01,  7.0310e-01, -1.1243e-01,  3.1637e-01,\n          1.5981e-01, -9.6211e-03, -1.0382e+00, -1.8747e-03, -9.9495e-02,\n         -1.5131e+00,  7.5719e-01, -7.7794e-02,  1.0319e+00,  5.2133e-01,\n         -3.2082e-01, -1.3737e-01,  1.0844e+00, -3.7648e-01, -3.4650e-02,\n          1.3097e-01, -3.5184e-01, -8.1428e-01,  3.4189e-01,  6.7282e-02,\n         -1.9175e-01,  2.2250e-01,  4.0790e-01, -4.0172e-02,  9.3394e-01,\n          1.4848e-01,  9.4150e-02, -6.1521e-01,  1.8073e-01, -9.3871e-01,\n         -3.5805e-01, -1.1437e-01,  9.8406e-01, -1.3756e+00,  9.7455e-02,\n         -2.3249e-01,  9.5018e-01, -2.1394e-01, -2.3394e-01,  5.0237e-01,\n          2.5898e-01, -2.2813e-01, -2.3680e-01, -2.3152e-01, -9.8057e-01,\n          3.9108e-01,  5.8315e-01,  1.6551e-01, -3.6449e-01,  4.2075e-01,\n          9.3581e-01, -4.6776e-01, -2.2666e-02,  3.7928e-01, -6.1125e-01,\n         -4.0730e-01,  9.0755e-01,  1.0523e+00, -1.9673e-01, -6.0428e-02,\n         -5.5663e-02,  3.8640e-01, -1.2758e-01,  6.1613e-01, -3.9228e-01,\n          9.0591e-01, -4.3536e-01, -7.4161e-02,  1.0847e-01, -6.4019e-02,\n          6.2278e-01,  3.7997e-01, -1.1579e-01, -1.9312e+00,  7.1141e-01,\n          1.1751e-01, -4.1557e-01, -7.7247e-01,  6.3692e-01,  5.3097e-01,\n          9.7168e-02, -6.8855e-02, -8.8752e-01,  4.2003e-01,  1.4736e-01,\n          4.4949e-01,  1.0757e-01,  8.5666e-01,  2.1895e-01, -1.4616e-01,\n         -2.1148e-01,  7.3091e-02,  5.6748e-01,  3.9416e-01,  2.8382e-02,\n          1.0420e+00, -9.9249e-02, -5.5125e-01,  7.3611e-02,  1.1771e+00,\n         -5.5362e-01, -1.0581e-01, -4.2232e-01, -1.5856e+00,  7.3779e-01,\n         -1.4219e-01, -1.0619e+00, -6.8308e-01,  1.3318e-02,  4.1730e-01,\n         -1.1350e+00,  2.5110e-01,  4.9541e-01,  1.0239e-01, -7.1889e-01,\n          1.0615e-01,  7.6836e-01, -6.0917e-02, -3.6846e-01,  5.1104e-02,\n         -6.9368e-01, -1.6377e-01,  7.2992e-01, -2.7181e-01, -1.7474e-01,\n          6.6675e-01, -2.4677e-01, -2.8554e-01, -7.7832e-02,  8.7495e-02,\n          2.1369e-01,  8.7279e-01, -9.8811e-02, -5.0639e-01, -4.2866e-01,\n         -5.1867e-01,  4.2720e-01,  3.1696e-01, -2.9805e-01, -8.3426e-01,\n         -1.0784e+00, -7.7276e-01,  4.9141e-01,  1.1272e+00,  1.3698e-02,\n         -6.8531e-02, -1.1509e-01, -6.5638e-01,  7.9699e-01, -2.6068e-01,\n          1.0395e+00, -4.7972e-01, -1.4439e-01,  7.8086e-01, -5.9054e-01,\n          2.1602e-01, -7.4450e-01, -1.3328e-01, -1.4613e-01,  8.9815e-01,\n         -1.0125e+00, -6.5561e-01,  7.6670e-01, -2.8419e-01, -1.3880e-01,\n         -7.1945e-01, -5.1779e-01,  1.4314e-01, -2.3534e-01, -5.9846e-01,\n          6.0434e-02, -6.3185e-02, -1.5664e+00, -1.9544e-01,  2.0409e-01,\n         -1.0337e+00,  9.1216e-01,  2.3952e-01,  1.0880e-01, -2.0045e-01,\n          8.4616e-01, -7.5020e-02,  3.4787e-01, -1.5094e+00, -2.5039e-01,\n         -6.5036e-02,  6.9634e-01, -2.6770e-01,  8.9710e-02, -4.8853e-01,\n          7.0874e-01, -7.6796e-01,  8.4987e-01,  4.1382e-01, -4.0460e-01,\n          2.8681e-01,  1.0482e+00,  1.6342e-01,  8.9449e-02, -2.9139e-01,\n         -6.0596e-01, -1.0153e-01, -3.3035e-01, -4.3888e-01, -6.9055e-02,\n          5.0943e-02,  3.7704e-01,  6.6890e-02,  5.8372e-01,  3.2395e-01,\n         -2.4983e-01, -2.9541e-01,  3.7929e-01, -3.1190e-01, -1.3260e-01,\n          5.0000e-01,  6.4270e-01, -3.0923e-01, -2.9641e-01, -8.8432e-01,\n          4.0610e-01,  6.5061e-01, -1.2178e-02,  1.1644e+00, -5.9447e-01,\n         -1.8064e-01,  6.1685e-01, -1.1272e-01, -6.0815e-01, -7.2103e-01,\n          7.6628e-01, -3.2992e-01, -5.0285e-01,  5.1562e-01, -4.1571e-01,\n         -4.7703e-01, -2.8721e-01,  8.4478e-01, -3.9162e-01,  9.2313e-02,\n          7.8206e-01,  2.1263e-01, -6.4606e-01,  9.6211e-01,  1.2251e-01,\n         -9.9896e-01,  1.4946e-01, -2.5206e-01, -3.9582e-01,  8.5691e-01,\n          3.5398e-01, -8.7718e-01,  2.3607e-01, -2.1513e-01, -6.1426e-01,\n         -1.4891e-01,  4.2167e-01, -9.3733e-01,  7.1058e-01, -1.5226e-01,\n          1.3321e+00,  1.2884e-01,  1.4089e-01,  6.7874e-01, -5.8004e-01,\n          1.5258e-01,  5.5926e-01, -1.4397e-01, -3.4066e-01, -6.2638e-01,\n          1.5209e-01,  1.1089e-01,  6.4280e-02,  5.5102e-01, -1.1813e-01,\n         -4.6605e-01,  3.0925e-01, -3.5231e-01,  4.1569e-01,  1.4494e+00,\n         -3.5271e-01, -9.2626e-01, -3.0457e-01, -2.8597e-01, -4.8825e-01,\n         -2.0081e-01,  6.2256e-02, -6.3340e-01,  4.3752e-02,  3.5119e-01,\n          7.5687e-01, -3.1523e-01,  2.9945e-01, -7.5232e-01,  1.2727e-01,\n         -5.1653e-01, -1.7894e-02,  4.0687e-01, -1.9404e-01,  3.7166e-01,\n          6.0207e-01,  9.7962e-01,  1.3890e-01,  6.2343e-01,  4.1314e-01,\n          4.1372e-01,  5.6574e-01, -4.6809e-01]], grad_fn=<DivBackward0>)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
