{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Calculating Similarity\n",
    "When calculating similarity between our transformer embedded vectors, we can use any of the three similarity metrics already covered.\n",
    "\n",
    "But first, let's create some embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/stsb-distilbert-base'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  101,  2093,  2086,  2101,  1010,  1996, 13123,  2001,  2145,  2440,\n",
      "         1997, 15333,  7174,  1012,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([  101,  1996,  3869, 13830,  1997, 13002,  1996,  3869, 18912,  2140,\n",
      "         1998,  2046,  1996, 11848,  2073,  2002,  2387,  2010,  2767,  2175,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([  101,  1996,  2711,  3482,  2001,  8966,  2007, 20919,  2116,  9877,\n",
      "         1997,  2706,  2101,  1012,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([ 101, 3061, 2006, 2028, 1005, 1055, 2132, 2012, 3105, 7636, 3596, 1037,\n",
      "        9879, 8605, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), tensor([ 101, 2009, 2165, 2032, 1037, 3204, 2000, 3926, 1996, 7954, 1012,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), tensor([  101,  2002,  2179,  1037,  3393, 28139,  7507,  4609,  1999,  2010,\n",
      "        18489,  5806,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])]\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "print(tokens['input_ids'])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "# torch.stack gabungkan tensor\n",
    "\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 128]),\n tensor([[  101,  2093,  2086,  2101,  1010,  1996, 13123,  2001,  2145,  2440,\n           1997, 15333,  7174,  1012,   102,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0],\n         [  101,  1996,  3869, 13830,  1997, 13002,  1996,  3869, 18912,  2140,\n           1998,  2046,  1996, 11848,  2073,  2002,  2387,  2010,  2767,  2175,\n           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0],\n         [  101,  1996,  2711,  3482,  2001,  8966,  2007, 20919,  2116,  9877,\n           1997,  2706,  2101,  1012,   102,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0],\n         [  101,  3061,  2006,  2028,  1005,  1055,  2132,  2012,  3105,  7636,\n           3596,  1037,  9879,  8605,  1012,   102,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0],\n         [  101,  2009,  2165,  2032,  1037,  3204,  2000,  3926,  1996,  7954,\n           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0],\n         [  101,  2002,  2179,  1037,  3393, 28139,  7507,  4609,  1999,  2010,\n          18489,  5806,  1012,   102,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0]]))"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape, tokens['input_ids']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "(odict_keys(['last_hidden_state']),\n BaseModelOutput(last_hidden_state=tensor([[[ 0.8595, -0.4167,  0.2726,  ...,  0.2641,  1.2247, -0.8213],\n          [ 0.7735,  0.1024,  0.6709,  ..., -0.0553,  0.4683, -1.2978],\n          [ 1.2928,  0.0555,  0.8122,  ..., -0.6352,  0.6269, -2.0369],\n          ...,\n          [ 0.5782, -0.0531,  0.1934,  ...,  0.2532,  0.9440, -0.6694],\n          [ 0.6663, -0.0837,  0.3519,  ...,  0.3226,  1.0445, -0.7756],\n          [ 0.9282, -0.0499,  0.4605,  ...,  0.3597,  0.9780, -0.7612]],\n \n         [[-0.1912, -0.0761, -0.2263,  ..., -0.2528, -0.4573,  1.2299],\n          [-0.1747, -0.2707,  0.0206,  ...,  0.2107, -0.7694,  0.7053],\n          [ 0.0048,  0.2234, -0.0399,  ...,  0.1220, -0.4588,  0.5472],\n          ...,\n          [-0.5226,  0.2483, -0.0838,  ..., -0.2564, -0.2952,  1.1068],\n          [-0.5535,  0.2634, -0.3198,  ..., -0.0839, -0.2407,  1.1977],\n          [-0.4390,  0.1172, -0.1790,  ..., -0.0739, -0.1049,  1.1897]],\n \n         [[-0.5386,  0.1490,  0.2582,  ...,  0.7013,  0.3570,  0.2032],\n          [-0.7112,  0.2646,  0.4560,  ...,  0.9502,  0.3710, -0.2759],\n          [-0.7087,  0.4556,  0.3564,  ...,  0.8394,  0.4145, -0.5526],\n          ...,\n          [-0.2382,  0.5677,  0.6094,  ...,  0.9512,  0.2190,  0.1587],\n          [-0.5645,  0.4406,  0.6393,  ...,  0.9134,  0.4077,  0.1677],\n          [-0.4842,  0.2677,  0.6652,  ...,  0.9003,  0.2893,  0.2075]],\n \n         [[ 0.1898, -0.1153,  0.0555,  ..., -0.5764,  0.1052,  0.2396],\n          [ 0.6943,  0.1323,  0.2454,  ..., -0.2863,  0.0768,  0.0305],\n          [ 0.2693,  0.2174,  0.1718,  ..., -0.5594,  0.8256,  0.0247],\n          ...,\n          [-0.0101, -0.2047,  0.4170,  ..., -0.3384,  0.1821,  0.1426],\n          [-0.0156, -0.3739,  0.2323,  ..., -0.3906,  0.4062, -0.1493],\n          [ 0.0199, -0.2943,  0.3118,  ..., -0.0719,  0.1518,  0.1194]],\n \n         [[-0.2488,  0.1792,  0.1492,  ..., -0.4932,  0.6305,  0.2019],\n          [ 0.1935, -0.2475,  0.3629,  ..., -0.4452,  0.8717,  0.2357],\n          [ 0.0536,  0.0813,  0.5912,  ..., -0.4718,  0.4109,  0.4894],\n          ...,\n          [-0.5690,  0.0568,  0.2596,  ..., -0.6615,  0.4418,  0.5430],\n          [-0.5817,  0.1804,  0.1706,  ..., -0.7117,  0.5911,  0.4262],\n          [-0.4825,  0.1645,  0.3929,  ..., -0.5731,  0.4300,  0.4576]],\n \n         [[-0.4679, -0.5711, -0.7151,  ..., -0.0855,  0.9365,  0.2650],\n          [-0.1452, -0.3075, -0.4660,  ..., -0.0350,  1.0357, -0.4554],\n          [-0.3416, -0.7703, -0.4885,  ..., -0.0628,  0.8810, -0.1210],\n          ...,\n          [-0.0935, -0.4681, -0.5666,  ...,  0.1114,  0.6993,  0.1984],\n          [-0.3984, -0.4246, -0.5261,  ...,  0.1612,  0.8649,  0.2393],\n          [-0.7443, -0.3460, -0.5605,  ..., -0.1556,  0.9270,  0.0508]]],\n        grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None))"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs.keys(), outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dense vector representations of our text are contained within the outputs 'last_hidden_state' tensor, which we access like so:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 128, 768]),\n tensor([[[ 0.8595, -0.4167,  0.2726,  ...,  0.2641,  1.2247, -0.8213],\n          [ 0.7735,  0.1024,  0.6709,  ..., -0.0553,  0.4683, -1.2978],\n          [ 1.2928,  0.0555,  0.8122,  ..., -0.6352,  0.6269, -2.0369],\n          ...,\n          [ 0.5782, -0.0531,  0.1934,  ...,  0.2532,  0.9440, -0.6694],\n          [ 0.6663, -0.0837,  0.3519,  ...,  0.3226,  1.0445, -0.7756],\n          [ 0.9282, -0.0499,  0.4605,  ...,  0.3597,  0.9780, -0.7612]],\n \n         [[-0.1912, -0.0761, -0.2263,  ..., -0.2528, -0.4573,  1.2299],\n          [-0.1747, -0.2707,  0.0206,  ...,  0.2107, -0.7694,  0.7053],\n          [ 0.0048,  0.2234, -0.0399,  ...,  0.1220, -0.4588,  0.5472],\n          ...,\n          [-0.5226,  0.2483, -0.0838,  ..., -0.2564, -0.2952,  1.1068],\n          [-0.5535,  0.2634, -0.3198,  ..., -0.0839, -0.2407,  1.1977],\n          [-0.4390,  0.1172, -0.1790,  ..., -0.0739, -0.1049,  1.1897]],\n \n         [[-0.5386,  0.1490,  0.2582,  ...,  0.7013,  0.3570,  0.2032],\n          [-0.7112,  0.2646,  0.4560,  ...,  0.9502,  0.3710, -0.2759],\n          [-0.7087,  0.4556,  0.3564,  ...,  0.8394,  0.4145, -0.5526],\n          ...,\n          [-0.2382,  0.5677,  0.6094,  ...,  0.9512,  0.2190,  0.1587],\n          [-0.5645,  0.4406,  0.6393,  ...,  0.9134,  0.4077,  0.1677],\n          [-0.4842,  0.2677,  0.6652,  ...,  0.9003,  0.2893,  0.2075]],\n \n         [[ 0.1898, -0.1153,  0.0555,  ..., -0.5764,  0.1052,  0.2396],\n          [ 0.6943,  0.1323,  0.2454,  ..., -0.2863,  0.0768,  0.0305],\n          [ 0.2693,  0.2174,  0.1718,  ..., -0.5594,  0.8256,  0.0247],\n          ...,\n          [-0.0101, -0.2047,  0.4170,  ..., -0.3384,  0.1821,  0.1426],\n          [-0.0156, -0.3739,  0.2323,  ..., -0.3906,  0.4062, -0.1493],\n          [ 0.0199, -0.2943,  0.3118,  ..., -0.0719,  0.1518,  0.1194]],\n \n         [[-0.2488,  0.1792,  0.1492,  ..., -0.4932,  0.6305,  0.2019],\n          [ 0.1935, -0.2475,  0.3629,  ..., -0.4452,  0.8717,  0.2357],\n          [ 0.0536,  0.0813,  0.5912,  ..., -0.4718,  0.4109,  0.4894],\n          ...,\n          [-0.5690,  0.0568,  0.2596,  ..., -0.6615,  0.4418,  0.5430],\n          [-0.5817,  0.1804,  0.1706,  ..., -0.7117,  0.5911,  0.4262],\n          [-0.4825,  0.1645,  0.3929,  ..., -0.5731,  0.4300,  0.4576]],\n \n         [[-0.4679, -0.5711, -0.7151,  ..., -0.0855,  0.9365,  0.2650],\n          [-0.1452, -0.3075, -0.4660,  ..., -0.0350,  1.0357, -0.4554],\n          [-0.3416, -0.7703, -0.4885,  ..., -0.0628,  0.8810, -0.1210],\n          ...,\n          [-0.0935, -0.4681, -0.5666,  ...,  0.1114,  0.6993,  0.1984],\n          [-0.3984, -0.4246, -0.5261,  ...,  0.1612,  0.8649,  0.2393],\n          [-0.7443, -0.3460, -0.5605,  ..., -0.1556,  0.9270,  0.0508]]],\n        grad_fn=<NativeLayerNormBackward0>))"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings.shape, embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 128]),\n tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]))"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape, attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 128, 768]),\n tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape, mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 128, 768]),\n tensor([[[ 0.8595, -0.4167,  0.2726,  ...,  0.2641,  1.2247, -0.8213],\n          [ 0.7735,  0.1024,  0.6709,  ..., -0.0553,  0.4683, -1.2978],\n          [ 1.2928,  0.0555,  0.8122,  ..., -0.6352,  0.6269, -2.0369],\n          ...,\n          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n \n         [[-0.1912, -0.0761, -0.2263,  ..., -0.2528, -0.4573,  1.2299],\n          [-0.1747, -0.2707,  0.0206,  ...,  0.2107, -0.7694,  0.7053],\n          [ 0.0048,  0.2234, -0.0399,  ...,  0.1220, -0.4588,  0.5472],\n          ...,\n          [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n \n         [[-0.5386,  0.1490,  0.2582,  ...,  0.7013,  0.3570,  0.2032],\n          [-0.7112,  0.2646,  0.4560,  ...,  0.9502,  0.3710, -0.2759],\n          [-0.7087,  0.4556,  0.3564,  ...,  0.8394,  0.4145, -0.5526],\n          ...,\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[ 0.1898, -0.1153,  0.0555,  ..., -0.5764,  0.1052,  0.2396],\n          [ 0.6943,  0.1323,  0.2454,  ..., -0.2863,  0.0768,  0.0305],\n          [ 0.2693,  0.2174,  0.1718,  ..., -0.5594,  0.8256,  0.0247],\n          ...,\n          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n \n         [[-0.2488,  0.1792,  0.1492,  ..., -0.4932,  0.6305,  0.2019],\n          [ 0.1935, -0.2475,  0.3629,  ..., -0.4452,  0.8717,  0.2357],\n          [ 0.0536,  0.0813,  0.5912,  ..., -0.4718,  0.4109,  0.4894],\n          ...,\n          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n \n         [[-0.4679, -0.5711, -0.7151,  ..., -0.0855,  0.9365,  0.2650],\n          [-0.1452, -0.3075, -0.4660,  ..., -0.0350,  1.0357, -0.4554],\n          [-0.3416, -0.7703, -0.4885,  ..., -0.0628,  0.8810, -0.1210],\n          ...,\n          [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n        grad_fn=<MulBackward0>))"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape, masked_embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 768]),\n tensor([[ 14.3088,  -2.9345,   8.4755,  ...,   1.2702,  13.5866, -17.3509],\n         [ -1.9532,   3.5471,  -1.8108,  ...,   3.3763,  -8.7374,  21.2288],\n         [ -3.7284,   5.2960,  10.8504,  ...,  12.8177,   1.2172,  -2.1437],\n         [  4.2427,   0.9002,   3.6707,  ...,  -7.6842,   4.1800,   1.5905],\n         [  1.5297,   4.1886,   4.5376,  ...,  -5.8318,   6.1333,   1.5439],\n         [ -2.2378,  -5.5957, -10.2656,  ...,   0.9247,  10.5801,   0.1131]],\n        grad_fn=<SumBackward1>))"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape, summed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 768]),\n tensor([[15., 15., 15.,  ..., 15., 15., 15.],\n         [22., 22., 22.,  ..., 22., 22., 22.],\n         [15., 15., 15.,  ..., 15., 15., 15.],\n         [16., 16., 16.,  ..., 16., 16., 16.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [14., 14., 14.,  ..., 14., 14., 14.]]))"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape, summed_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.9539, -0.1956,  0.5650,  ...,  0.0847,  0.9058, -1.1567],\n        [-0.0888,  0.1612, -0.0823,  ...,  0.1535, -0.3972,  0.9649],\n        [-0.2486,  0.3531,  0.7234,  ...,  0.8545,  0.0811, -0.1429],\n        [ 0.2652,  0.0563,  0.2294,  ..., -0.4803,  0.2613,  0.0994],\n        [ 0.1275,  0.3491,  0.3781,  ..., -0.4860,  0.5111,  0.1287],\n        [-0.1598, -0.3997, -0.7333,  ...,  0.0661,  0.7557,  0.0081]],\n       grad_fn=<DivBackward0>)"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's calculate cosine similarity for sentence 0:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Program mean_pooled.detach().numpy() berfungsi untuk mengambil nilai array numpy dari tensor yang telah di-detach dan dihitung nilai rata-ratanya (mean pooled).\n",
    "\n",
    "Penjelasan lebih detailnya:\n",
    "\n",
    "- mean_pooled: variabel yang berisi tensor dengan nilai rata-rata (mean) setiap kolomnya.\n",
    "- .detach(): metode untuk menghapus koneksi tensor dari graf komputasi sehingga tidak mempengaruhi perhitungan backward gradient.\n",
    "- .numpy(): metode untuk mengubah tensor menjadi array numpy.\n",
    "\n",
    "Dengan demikian, mean_pooled.detach().numpy() akan menghasilkan array numpy dengan nilai rata-rata setiap kolom dari tensor mean_pooled.\n",
    "\n",
    "Menghapus koneksi tensor dari graf komputasi mengacu pada operasi pemutusan hubungan antara tensor dan graf komputasi yang digunakan untuk menghitung gradien dalam proses backpropagation pada jaringan saraf. Ketika kita memutuskan hubungan antara tensor dan graf komputasi, kita menghilangkan jejak koneksi tensor yang terkait dengan proses pembuatan graf tersebut. Hal ini dapat membantu kita mengurangi penggunaan memori dan meningkatkan efisiensi komputasi pada jaringan saraf yang kompleks. Selain itu, dengan memutuskan hubungan tensor dari graf komputasi, kita juga dapat memproses tensor secara independen dan mengoptimalkan performa dari perangkat keras seperti GPU dan TPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.04432429,  0.29033935, -0.00305667,  0.02244765,  0.2989486 ]],\n      dtype=float32)"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "\n",
    "cosine = cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")\n",
    "\n",
    "cosine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "cosine_similarity adalah sebuah fungsi yang digunakan untuk menghitung cosine similarity antara dua set vektor. Cosine similarity adalah sebuah metrik yang digunakan untuk mengukur seberapa mirip dua set vektor dalam ruang dimensi yang sama. Semakin dekat nilai cosine similarity antara dua vektor ke 1, semakin mirip kedua vektor tersebut. Fungsi cosine_similarity dapat digunakan untuk mengukur kesamaan antara dua set dokumen atau teks, sehingga berguna dalam proses information retrieval atau natural language processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Sentence\": sentences[1:],\n",
    "    \"Similarity\": cosine[0] # to replace from array() tuple / next(iter(cosine))\n",
    "}\n",
    "\n",
    "create_table = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            Sentence  Similarity\n0  The fish dreamed of escaping the fishbowl and ...    0.044324\n1  The person box was packed with jelly many doze...    0.290339\n2  Standing on one's head at job interviews forms...   -0.003057\n3            It took him a month to finish the meal.    0.022448\n4         He found a leprechaun in his walnut shell.    0.298949",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The fish dreamed of escaping the fishbowl and ...</td>\n      <td>0.044324</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The person box was packed with jelly many doze...</td>\n      <td>0.290339</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Standing on one's head at job interviews forms...</td>\n      <td>-0.003057</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It took him a month to finish the meal.</td>\n      <td>0.022448</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>He found a leprechaun in his walnut shell.</td>\n      <td>0.298949</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, as intended, the most similar sentence is that in index 2 - which contains the same meaning as our first sentence, without using the same words:\n",
    "\n",
    "\"Three years later, the coffin was still full of Jello.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
